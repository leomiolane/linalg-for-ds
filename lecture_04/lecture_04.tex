\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}
\externaldocument{../lecture_02/lecture_02}


\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Lecture 4: Norm and inner product}
\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\input{../preamble.tex}

\section{Norm}

\begin{definition}[Norm]
	Let $V$ be a vector space.
	A norm $\| \cdot \|$ on $V$ is a function from $V$ to $\R_{\geq 0}$ that verifies the following points:
	\begin{enumerate}[label=(\roman*)]
		\item \emph{Homogeneity}: $\| \alpha v \| = |\alpha|\times \| v\|$ for all $\alpha \in \R$ and  $v \in V$.
		\item \emph{Triangular inequality}: $\|u + v\| \leq \|u\| + \|v\|$ for all $u,v \in V$.
		\item \emph{Positive definiteness}: if $\|v\| = 0$ for some $v \in V$, then $v=0$.
	\end{enumerate}
\end{definition}

\begin{example}
	One can consider various norms over $\R^n$:
	\begin{itemize}
		\item The Euclidean norm $\|x\|_2 \defeq \sqrt{\sum_{i=1}^n x_i^2}$.
		\item The $\ell_1$ norm $\|x\|_1 \defeq \sum_{i=1}^n |x_i|$.
		\item More generally, given $p \geq 1$, the $\ell_p$-norm $\|x\|_p \defeq \big(\sum_{i=1}^n |x_i|^p \big)^{1/p}$.
		\item The infinity-norm $\|x \|_{\infty} \defeq \max(|x_1|, \dots, |x_n|)$.
	\end{itemize}
\end{example}


\section{Inner product}

\begin{definition}[Inner product]
	Let $V$ be a vector space.
	An inner product on $V$ is a function $\langle \cdot, \cdot \rangle$ from $V \times V$ to $\R$ that verifies the following points:
	\begin{enumerate}[label=(\roman*)]
		\item \emph{Symmetry}: $\langle u, v \rangle = \langle v, u\rangle$ for all $u,v \in V$.
		\item \emph{Linearity}: $\langle u+v, w \rangle = \langle u, w\rangle + \langle v, w\rangle$ and $\langle \alpha v, w \rangle = \alpha \langle v, w \rangle$ for all $u,v,w \in V$ and $\alpha \in \R$.
		\item \emph{Positive definiteness}: $\langle v, v\rangle \geq 0$ with equality if and only if $v = 0$.
	\end{enumerate}
\end{definition}

\begin{example}
	\leavevmode
	\begin{itemize}
		\item For $V = \R^n$, the Euclidean inner product $\langle x,y\rangle = \sum_{i=1}^n x_i y_i = x^{\sT} y$ is an inner product.
		\item If $V$ is the set of all continuous functions on $[0,1]$, then $\langle f,g\rangle = \int_0^1 f(t) g(t) dt$ is an inner product.
	\end{itemize}
\end{example}

\begin{proposition}[Norm induced by an inner product]
	If $\langle \cdot, \cdot \rangle$ is an inner product on $V$ then $\| v \| \defeq \sqrt{\langle v, v \rangle}$ is a norm on $V$. We say  that the norm $\| \cdot \|$ is induced by the inner product $\langle \cdot, \cdot \rangle$.
\end{proposition}

\begin{remark}
	The Euclidean norm $\| \cdot \|_2$ on $V=\R^n$ is induced by the Euclidean inner product $x \cdot y = \sum_{i=1}^n x_i y_i$. Indeed, for $x \in \R^n$,
	$$
	\|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2} = \sqrt{x \cdot x}.
	$$
\end{remark}
\begin{exercise}
	Let $\langle \cdot, \cdot \rangle$ be an inner product on $\R^n$, and let $\| \cdot \|$ be the induced norm by $\langle \cdot, \cdot \rangle$.
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		\item Show that for all $x,y \in \R^n$ we have
			$$
			\|x+y\|^2 + \|x-y\|^2 = 2 \|x\|^2 + 2 \|y\|^2.
			$$
		\item Deduce from the previous question that the $\ell_1$ norm $\| \cdot \|_1$ and the infinity norm $\| \cdot \|_{\infty}$ are \textbf{not} induced by an inner product.
	\end{enumerate}
\end{exercise}

\begin{theorem}[Cauchy-Schwarz inequality]
	Let $\| \cdot \|$ be the norm induced by the inner product $\langle \cdot , \cdot \rangle$ on the vector space $V$. Then for all $x,y \in V$:
	\begin{equation}\label{eq:cauchy_schwarz}
	| \langle x,y \rangle | \leq \|x\| \, \|y\|.
	\end{equation}
	Moreover, there is equality in \eqref{eq:cauchy_schwarz} if and only if $x$ and $y$ are linearly dependent, i.e.\ $x = \alpha y$ or $y = \alpha x$ for some $\alpha \in \R$.
\end{theorem}
\begin{proof}
	If $x = 0$ or $y = 0$ the result is obvious, we assume therefore to be in the case where $x \neq 0$ and $y \neq 0$.
	For $t \in \R$ we define the function $f(t) = \|t x - y \|^2$. Since the norm $\| \cdot \|$ is induced by the inner product $\langle \cdot, \cdot \rangle$ we have
	$$
	f(t) = \langle t x - y , t x - y \rangle = t^2 \|x\|^2 - 2t \langle x,y\rangle + \|y\|^2.
	$$
	$f$ is therefore a quadratic function of $t$. Notice that $f$ is non-negative because $f(t)= \|t x - y \|^2 \geq 0$. This gives that its discriminant $\Delta$ is non-positive:
	$$
	\Delta =  (2 \langle x,y \rangle)^2 - 4 \|x\|^2 \|y\|^2 \leq 0,
	$$
	which proves \eqref{eq:cauchy_schwarz}. We have equality in \eqref{eq:cauchy_schwarz} if and only if $\Delta = 0$ that is if and only if $f$ admits a zero $\alpha$, which is equivalent to $\alpha x - y = 0$, i.e.\ $y = \alpha x$.
\end{proof}

\section{Orthogonality}

In this section we consider an inner product $\langle \cdot, \cdot \rangle$ (that induces a norm $\| \cdot \|$) on a vector space $V$. For simplicity one may think of $\langle \cdot, \cdot \rangle$ and $\| \cdot \|$ to be the usual Euclidean dot product and norm on $V = \R^n$.

\begin{definition}[Orthogonality]
	\begin{itemize}
		\item We say that vectors $x$ and $y$ are \emph{orthogonal} if $\langle x,y \rangle = 0$. We write then $x \perp y$.
		\item We say that a vector $x$ is orthogonal to a set of vectors $A \subset V$ if $x$ is orthogonal to all the vectors in $A$, i.e.\ $\forall y \in A, \ \langle x,y\rangle = 0$. We write then $x \perp A$.
		\item More generality we say that $A \subset V$ and $B \subset V$ are orthogonal if $\langle x,y \rangle = 0$ for all $x \in A$ and all $y \in B$. As before, we write $A \perp B$.
	\end{itemize}
\end{definition}


%\begin{proposition}
	%If $x$ is orthogonal to $v_1, \dots, v_k$ then $x$ is orthogonal to any linear combination of these vectors i.e.\ $x \perp \Span(v_1, \dots, v_k)$.
%\end{proposition}

\begin{theorem}[Pythagorean theorem]
	Let $x,y \in V$. Then
	$$
	x \perp y \ \Longleftrightarrow \|x+y\|^2 = \|x\|^2 + \|y\|^2.
	$$
\end{theorem}


\begin{definition}[Orthogonal and orthonormal families of vectors]
	Let $v_1, \dots, v_k$ be vectors of $V$. We say that the family of vectors $(v_1, \dots, v_k)$ is
	\begin{itemize}
		\item \emph{orthogonal} if the vectors $v_1, \dots, v_n$ are pairwise orthogonal, i.e.\ $\langle v_i, v_j \rangle = 0$ for all $i \neq j$.
		\item \emph{orthonormal} if it is orthogonal and if all the $v_i$ have unit norm: $\|v_1\| = \dots = \|v_k\| = 1$.
	\end{itemize}
\end{definition}

Orthonormal basis are particularly convenient for computing coordinates of vectors:

\begin{proposition}\label{prop:coords}
	Assume that $\dim(V)=n$ and let $(v_1, \dots, v_n)$ be an \textbf{orthonormal} basis of $V$. Then the coordinates of a vector $x \in V$ in the basis $(v_1, \dots, v_n)$ are $(\langle v_1, x\rangle, \dots, \langle v_n,x \rangle)$:
	$$
	x = \langle v_1, x \rangle v_1 + \cdots + \langle v_n, x \rangle v_n.
	$$
	Moreover
	$$
	\|x\| = \sqrt{\langle v_1, x \rangle^2 + \cdots + \langle v_n, x \rangle^2}.
	$$
\end{proposition}

\section{Orthogonal projection and distance to a subspace}

We assume in this section that $V = \R^n$ and that $\langle \cdot, \cdot \rangle$, $\| \cdot \|$ are respectively the Euclidean dot product and Euclidean norm.

\begin{definition}[Orthogonal projection and distance to a subspace]
	Let $S$ be a subspace of $\R^n$. The \emph{orthogonal projection} of a vector $x$ onto $S$ is defined as the vector $P_S(x)$ is $S$ that minimizes the distance to $x$:
	$$
	P_S(x) \defeq \argmin_{y \in S} \|x - y\|.
	$$
	The distance of $x$ to the subspace $S$ is then defined as
	$$
	d(x,S) \defeq \min_{y \in S} \| x - y \| = \|x - P_S(x)\|.
	$$
\end{definition}

\begin{proposition}\label{prop:ortho}
	Let $S$ be a subspace of $\R^n$ and let $(v_1, \dots, v_k)$ be an \textbf{orthonormal basis} of $S$. Then for all $x \in \R^n$,
		$$P_S(x) = \langle v_1, x \rangle v_1 + \cdots + \langle v_k, x \rangle v_k.$$
		In other words, if we let
		$$
		V = 
\begin{pmatrix}
	| & | & & | \\
	v_1 & v_2 & \cdots& v_k \\
	| & | & & |
\end{pmatrix}
\in \R^{n \times k},
		$$
		then $P_S$ is a linear transformation whose matrix is $VV^{\sT}$:
		$$
		\forall x \in \R^n, \quad P_S(x) = V V^{\sT} x.
		$$
\end{proposition}
\begin{proof}
	Let us add vectors $v_{k+1}, \dots, v_n$ to the basis $(v_1, \dots, v_k)$ to obtain an orthonormal basis of $\R^n$. (This is made possible by the Gram-Schmidt orthonormalization principle that we will see in the next lecture.)
	Let $\alpha_1 = \langle x, v_1 \rangle, \dots, \alpha_n = \langle x , v_n \rangle$ be the coordinates of $x$ in the basis $(v_1, \dots, v_n)$.
	Let $y \in S$, and let $\beta_1, \dots, \beta_k$ be its coordinates in the basis $(v_1, \dots, v_k)$. By Proposition \ref{prop:coords}:
	$$
	\|x-y\|^2 = \sum_{i=1}^k (\alpha_i - \beta_i)^2 + \sum_{i=k+1}^n \alpha_i^2.
	$$
	Minimizing this quantity over $y \in S$ is equivalent to minimizing it over the coordinates $\beta_1, \dots, \beta_k$ of $y$. The minimum is uniquely achieved for $\beta_i = \alpha_i$ for all $i$, hence
	$$
	P_S(x) \defeq \argmin_{y \in S} \|x - y\|
	= \alpha_1 v_1 + \dots + \alpha_k v_k
	= \langle v_1, x \rangle v_1 + \dots + \langle v_k, x \rangle v_k.
	$$
	The second part of the proposition is a rewritting of this last equation, obtained by noticing that
	$$
	V^{\sT} x 
	= 
\begin{pmatrix}
	- \, v_1 \, -\\
	\vdots\\
	- \, v_k \,-
\end{pmatrix}
x
=
\begin{pmatrix}
	\langle v_1, x \rangle \\
	\vdots \\
	\langle v_k, x \rangle
\end{pmatrix}.
	$$
\end{proof}

\begin{corollary}\label{cor:projection}
	For all $x \in \R^n$,
	\begin{itemize}
		\item $x - P_S(x)$ is orthogonal to $S$.
		\item $\|P_S(x) \| \leq \|x\|$.
	\end{itemize}
\end{corollary}


\begin{definition}[Orthogonal complement]
	Let $S$ be a subspace of $\R^n$. The orthogonal complement of $S$ is defined by
	$$
	S^{\perp} \defeq 
	\big\{ x \in \R^n \, \big| \, x \perp S \big\} = 
	\big\{ x \in \R^n \, \big| \, \forall y \in S, \, \langle x,y \rangle = 0 \big\}.
	$$
\end{definition}

\begin{proposition}
	Let $S$ be a subspace of $\R^n$. Then $S^{\perp}$ is also a subspace of $\R^n$ with dimension
	$$
	\dim(S^{\perp}) = n - \dim(S).
	$$
\end{proposition}

\vspace{1cm}
\centerline{\pgfornament[width=7cm]{71}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
