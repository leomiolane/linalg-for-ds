\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}

\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Lecture 1: Vector spaces}
\author{LÃ©o \textsc{Miolane} \ $\cdot$ \ \texttt{leo.miolane@gmail.com}}
\date{\today}

\begin{document}
\maketitle
\input{../preamble.tex}
\section{General definitions}

We present below the abstract mathematical definition of a vector space. 
\textbf{Please do not try to memorize it!} 
Simply remember that a vector space is a set whose elements are called \emph{vectors}, that one can add vectors together and multiply them by real numbers called \emph{scalars}.
\begin{definition}[Vector space]
	A vector space (over $\R$) consists of of a set $V$ (whose elements are called vectors) and two operations $+$ and $\cdot$ that verify:
	\begin{enumerate}
		\item The sum of two vectors is a vector: for all $\vec{x},\vec{y} \in V$ we have $\vec{x}+\vec{y} \in V$.
		\item The vector sum is commutative and associative. For all $\vec{x},\vec{y},\vec{z} \in V$ we have
			$$
			\vec{x}+\vec{y} = \vec{y} + \vec{x} \qquad \text{and} \qquad \vec{x} + (\vec{y} + \vec{z}) = (\vec{x}+\vec{y}) + \vec{z}.
			$$
		\item There exists a zero vector $\vec{0} \in V$ that verifies $\vec{x} + \vec{0} = \vec{x}$ for all $\vec{x} \in V$.
		\item For all $\vec{x} \in V$, there exists $\vec{y} \in V$ such that $\vec{x} + \vec{y} = \vec{0}$. Such $\vec{y}$ is called the additive inverse of $\vec{x}$ and is written $- \vec{x}$.
		\item Scalar multiplication: for all $\vec{x} \in V$ and all $\alpha \in \R$, $\alpha \cdot \vec{x} \in V$.
		\item Identity element for scalar multiplication: $1 \cdot \vec{x} = \vec{x}$ for all $\vec{x} \in V$.
		\item Compatibility between scalar multiplication and the usual multiplication: for all $\alpha,\beta \in \R$ and all $\vec{x} \in V$, we have
			$$
			\alpha \cdot(\beta \cdot \vec{x}) = (\alpha \beta) \cdot \vec{x}.
			$$
		\item Distributivity: for all $\alpha,\beta \in \R$ and all $\vec{x},\vec{y} \in V$,
			$$
			(\alpha + \beta) \cdot \vec{x} = \alpha \cdot \vec{x} + \beta \cdot \vec{y}
			\qquad \text{and} \qquad
			\alpha \cdot (\vec{x} + \vec{y}) = \alpha \cdot \vec{x} + \alpha \cdot \vec{y}.
			$$
	\end{enumerate}
\end{definition}

From now we will ignore $\cdot$ and simply write $\alpha \vec{x}$ instead of $\alpha \cdot \vec{x}$. We will also remove the arrows and write $x$ instead of $\vec{x}$.

\begin{example}
	\leavevmode
	\begin{itemize}
		\item The set $V = \R^n$ endowed with the usual vector addition $+$
			$$
			(x_1, \dots, x_n) + (y_1, \dots, y_n) = (x_1 + y_1, \dots, x_n + y_n)
			$$
			and the usual scalar multiplication $\cdot$
			$$
			\alpha \cdot (x_1, \dots, x_n)= (\alpha x_1, \dots, \alpha x_n)
			$$
			is a vector space.
		\item The set $V = \cF(\R,\R) \defeq \{ f \, | \, f: \R \to \R \}$ of all functions from $\R$ to itself endowed with the addition $+$ and the scalar multiplication $\cdot$ defined by
			$$
			\begin{array}{rlll}
				f+g :& \R & \to & \R \\
					 &t & \mapsto & f(t) + g(t)
			\end{array}
			\qquad
			\text{and}
			\qquad
			\begin{array}{rlll}
				\alpha \cdot f :& \R & \to & \R \\
								&t & \mapsto & \alpha f(t)
			\end{array}
			$$
			is a vector space.
		\item The set of random variables on a given probability space is a vector space: if $X$ and $Y$ are two random variables and $\alpha \in \R$, $X+Y$ and $\alpha X$ are also random variables.
	\end{itemize}
\end{example}

\begin{definition}[Subspace]
	We say that a non-empty subset $S$ of a vector space $V$ is a \emph{subspace} if it is closed under addition and multiplication by a scalar, that is if
	\begin{enumerate}[label=(\roman*),noitemsep]
		\item for all $x,y \in S$ we have $x + y \in S$,
		\item for all $x \in S$ and all $\alpha \in \R$ we have $\alpha x \in S$.
	\end{enumerate}
\end{definition}

Notice that a subspace is also a vector space!
\begin{exercise}
	Show that every subspace $S$ of a vector space $V$ contains the zero vector $0$.
\end{exercise}

\section{Linear dependency}

\begin{definition}[Linear combination]
	Let $V$ be a vector space and $x_1, \dots, x_k \in V$. We say that $y \in V$ is a \emph{linear combination} of the vectors $x_1, \dots, x_k$ if there exists $\alpha_1, \dots, \alpha_k \in \R$ such that
	$$
	y = \sum_{i=1}^k \alpha_i x_i.
	$$
\end{definition}

Remember that a linear combination is always a \emph{finite} sum.

\begin{remark}
	If $S$ is a subspace of a vector space $V$, any linear combination of elements of $S$ belongs to $S$.
\end{remark}

\begin{definition}[Span]
	%Let $V$ be a vector space and $A \subset V$. The \emph{linear span} of $A$ is the set of all linear combinations of elements of $A$:
	%$$
	%\Span(A) = 
	%\Big\{ y \, \Big| \, \exists k \in \N, \, x_1, \dots, x_k \in A, \, \alpha_1, \dots, \alpha_k \in \R, \, y = \sum_{i=1}^k \alpha_i x_i \Big\}.
	%$$
	Let $x_1, \dots, x_k$ be vectors in a vector space $V$. We define the \emph{linear span} of $x_1, \dots, x_k$ as the set of all linear combinations of these vectors:
$$
\Span(x_1, \dots, x_k) \defeq
\Big\{ \alpha_1 x_1 + \cdots + \alpha_k x_k \, \Big| \, \alpha_1, \dots, \alpha_k \in \R \Big\}.
$$
\end{definition}
%Given vectors $x_1, \dots x_k \in V$ we will simply write
%$$
%\Span(x_1, \dots, x_k) = \Span(\{x_1, \dots, x_k \}) =
%\Big\{ \alpha_1 x_1 + \cdots + \alpha_k x_k \, \Big| \, \alpha_1, \dots, \alpha_k \in \R \Big\}.
%$$

One can easily verify (exercise!) that $\Span(x_1,\dots,x_k)$ is a subspace of $V$. One can also verify (exercise!) that
$$
\Span(x_1, \dots, x_k) = \bigcap_{\substack{S \ \text{subspace of} \ V \\ x_1, \dots, x_k \in S}} S,
$$
$\Span(x_1,\dots,x_k)$ is therefore the \textbf{smallest} (for the inclusion $\subset$) \textbf{subspace of $S$ that contains $x_1,\dots,x_k$}.

\begin{definition}[Linear dependency]
	Vectors $x_1, \dots x_k \in V$ are \emph{linearly dependent} is there exists $\alpha_1, \dots, \alpha_k \in \R$ \textbf{that are not all zero} such that 
	$$
	\alpha_1 x_1 + \cdots + \alpha_k x_k = 0.
	$$
	They are said to be \emph{linearly independent} otherwise.
\end{definition}

Saying that $x_1, \dots, x_k$ are linearly dependent precisely means that one of the vectors $x_1, \dots, x_k$ can be obtained as a linear combination of the others. 
Indeed if $x_1, \dots, x_k$ are linearly dependent, then we can find $\alpha_1, \dots, \alpha_k \in \R$ that are not all zero (there exists $i$ such that $\alpha_i \neq 0$) such that $\alpha_1 x_1 + \cdots + \alpha_k x_k = 0$. This leads to
$$
x_i = \sum_{j \neq i} \frac{- \alpha_j}{\alpha_i} x_j,
$$
i.e.\ the vector $x_i$ can be expressed as a linear combinations of the vectors $x_j$ for $j \neq i$.
Conversely if we have for some $i$, and $\alpha_1, \dots, \alpha_k \in \R$
$$
x_i = \alpha_1 x_1 + \cdots + \alpha_{i-1} x_{i-1} + \alpha_{i+1} x_{i+1} + \cdots \alpha_k x_k = 0.
$$
then $\alpha_1 x_1 + \cdots + \alpha_{i-1} x_{i-1} - x_i + \alpha_{i+1} x_{i+1} + \cdots \alpha_k x_k = 0$ which gives that $x_1, \dots, x_k$ are linearly dependent. 


\section{Basis, dimension}

\subsection{Definitions}
\begin{definition}[Basis]\label{def:basis}
	A family $(x_1, \dots, x_n)$ of vectors of $V$ is a basis of $V$ if
	\begin{enumerate}[label=(\roman*)]
		\item $x_1, \dots, x_n$ are linearly independent,
		\item $\Span(x_1, \dots, x_n) = V$.
	\end{enumerate}
\end{definition}

\begin{definition}[Dimension]
	Let $V$ be a vector space.
	\begin{itemize}
		\item If $V$ admits a basis $(v_1, \dots, v_n)$, then every basis of $V$ has also $n$ vectors. We say that $V$ has dimension $n$ and write $\dim(V) = n$.
		\item Otherwise, we say that $V$ has infinite dimension: $\dim(V) = +\infty$.
	\end{itemize}
\end{definition}
The dimension is therefore the minimum number of vector needed to span the vector space.
In this course we are going to focus mostly on finite dimensional spaces and $\R^n$ in particular.
\\

\begin{proof}
	We will need the following lemma:
	\begin{lemma}\label{lem:lin_dep}
		Let $v_1, \dots, v_n \in V$ and suppose that we have vectors $x_1, \dots, x_k \in V$ such that $k > n$ and $x_i \in \Span(v_1, \dots, v_n)$ for all $i \in \{1, \dots, k\}$.
		Then $x_1, \dots, x_k$ are linearly dependent.
	\end{lemma}
	Lemma \ref{lem:lin_dep} will be proved at the end of the notes.
	We proceed by contradiction and assume that there exists two basis $(v_1, \dots, v_n)$ and $(x_1, \dots, x_k)$ of $V$ such that $k \neq n$. Without loss of generality we can assume that $k > n$. For $i = 1, \dots, k$ we have
	$$
	x_i \in V = \Span(v_1, \dots, v_n),
	$$
	because $(v_1, \dots, v_n)$ is a basis of $V$. We can therefore apply Lemma \ref{lem:lin_dep} to get that $x_1, \dots, x_{n+1}$ are linearly dependent. This contradicts the fact that $(x_1, \dots, x_{k})$ is a basis.
\end{proof}
\\

\begin{example}
	Let us define the vectors $e_1, \dots, e_n \in \R^n$ by
	\begin{align*}
		e_1 &= (1, 0, 0, \dots, 0) \\
		e_2 &= (0, 1, 0, \dots, 0) \\
		\vdots & \\
		e_n &= (0, 0, 0, \dots, 1).
	\end{align*}
	One can verify (exercise!) that the family $(e_1, \dots, e_n)$ is a basis of $\R^n$. This basis is called the ``canonical basis'' of $\R^n$. We conclude that $\R^n$ has dimension $n$.
\end{example}
\vspace{0.5cm}

\begin{definition}[Lines, hyperplanes]
	Let $S$ be a subspace of $\R^n$.
	\begin{itemize}
		\item We call $S$ a \emph{line} if $\dim(S) = 1$.
		\item We call $S$ an \emph{hyperplane} if $\dim(S) = n-1$.
	\end{itemize}
\end{definition}

\subsection{Some useful properties}

\begin{proposition}\label{prop:cara}
	Let $V$ be a vector space that has dimension $\dim(V) = n$. Then
	\begin{itemize}
		\item Any family of vectors of $V$ that are linearly independent contains at most $n$ vectors (i.e.\ if $x_1, \dots, x_k \in V$ are linearly independent, then $k \leq n$).
		\item Any family of vectors of $V$ that spans $V$ contains at least $n$ vectors (i.e.\ if $x_1, \dots, x_k \in V$ are such that $\Span(x_1, \dots, x_k) = V$, then $k \geq n$).
	\end{itemize}
\end{proposition}
\begin{exercise}
	Prove Proposition \ref{prop:cara} using Lemma \ref{lem:lin_dep}.
\end{exercise}

The next proposition is very useful for proving that a family $(x_1, \dots, x_n)$ is a basis of a vector space $V$ of dimension $n$:
\begin{proposition}\label{prop:card}
		Let $V$ be a vector space of dimension $n$ and let $x_1, \dots, x_n \in V$.
		\begin{enumerate}
			\item If $x_1, \dots, x_n$ are linearly independent, then $(x_1, \dots, x_n)$ is a basis of $V$.
			\item If $\Span(x_1, \dots, x_n) = V$, then $(x_1, \dots, x_n)$ is a basis of $V$.
		\end{enumerate}
	\end{proposition}

	In words, if the family we consider has as many vectors as the dimension of $V$ then we only have to check one of the two points of Definition \ref{def:basis}!

	\begin{proposition}\label{prop:ineq_dim}
	Let	$U$ and $V$ be two subspaces of $\R^n$. Assume that $U \subset V$. Then
	$$
	\dim(U) \leq \dim(V) \leq n.
	$$
	If \textbf{moreover} $\dim(U) = \dim(V)$, then $U = V$.
\end{proposition}
\begin{exercise}
	Prove Proposition \ref{prop:ineq_dim} using Propositions \ref{prop:cara} and \ref{prop:card}.
\end{exercise}


\begin{proposition}[Coordinates]
	Let $(v_1, \dots, v_n)$ be a basis of $V$. Then for every $x \in V$ there exists a unique vector $(\alpha_1, \dots, \alpha_n) \in \R^n$ such that
	$$
	x = \alpha_1 v_1 + \dots + \alpha_n v_n.
	$$
	We say that $(\alpha_1, \dots, \alpha_n)$ are the coordinates of $x$ in the basis $(v_1, \dots, v_n)$.
\end{proposition}
\begin{proof}
	\textbf{Existence}. $(v_1, \dots, v_n)$ forms a basis of $V$ therefore  $V = \Span(v_1, \dots, v_n)$. We get that $x \in \Span(v_1, \dots, v_n)$ which gives that there exists $\alpha_1, \dots, \alpha_n$ such that $x = \alpha_1 v_1 + \cdot + \alpha_n v_n$.
	\\
	\textbf{Uniqueness}. Let $\alpha_1, \dots, \alpha_n, \beta_1, \dots, \beta_n \in \R$ such that
	\begin{align*}
		x &= \alpha_1 v_1 + \cdots + \alpha_n v_n = \beta_1 v_1 + \cdots + \beta_n v_n.
	\end{align*}
	This leads to
	$$
	(\alpha_1 - \beta_1) v_1 + \cdots + (\alpha_n - \beta_n) v_n = 0.
	$$
	The vectors $v_1, \dots, v_n$ are linearly independent because they forms a basis. Consequently $\alpha_1 - \beta_1 = \alpha_2 - \beta_2 = \cdots =  \alpha_n - \beta_n = 0$, i.e.\ $(\alpha_1, \dots, \alpha_n) = (\beta_1, \dots, \beta_n)$.
\end{proof}


\section*{Proof of Lemma \ref{lem:lin_dep}}\label{sec:proof_lin_dep}
Notice that it suffices to prove the theorem for $k = n+1$ because if $x_1, \dots, x_{n+1}$ are linearly dependent, so are $x_1, \dots, x_{n+1}, \dots x_k$. We will therefore show for all $n \geq 1$
\begin{align*}
	\mathcal{H}_n: \text{<< } &\text{For all } v_1, \dots, v_{n} \in V \ \text{and all } x_{1}, \dots x_{n+1} \in \Span(v_1, \dots, v_n), \\ &\text{the vectors } x_1, \dots, x_{n+1} \ \text{are linearly dependent. >>}
\end{align*}
\textbf{Base case:} $\mathcal{H}_1$ is true. Indeed, if $x_1,x_2 \in \Span(v_1)$, then there exists $\alpha_1, \alpha_2 \in \R$ such that $x_1 = \alpha_1 v_1$ and $x_2 = \alpha_2 v_1$. If $\alpha_1 = 0$ then $x_1 = 0$ and $x_1,x_2$ are therefore linearly dependent. 
Otherwise if $\alpha_1 \neq 0$ then $v_1 = \frac{1}{\alpha_1} x_1$ which then gives $x_2 = \frac{\alpha_2}{\alpha_1} x_1$: $x_1,x_2$ are linearly dependent. 
\\

\noindent\textbf{Induction step:} We assume now that $\mathcal{H}_{n-1}$ holds for some $n \geq 2$ and we will show that $\mathcal{H}_{n}$ holds. We consider therefore $x_1, \dots, x_{n+1} \in \Span(v_1, \dots, v_n)$. We can find real numbers $\alpha_{i,j}$ such that
$$
\begin{array}{ccccccc}
	x_1 &=& \alpha_{1,1} v_1 &+& \cdots &+& \alpha_{1,n} v_n\\
	x_2 &=& \alpha_{2,1} v_1 &+& \cdots &+& \alpha_{2,n} v_n \\
	\vdots &&&&&& \\
	x_{n+1} &=& \alpha_{n+1,1} v_1 &+& \cdots &+& \alpha_{n+1,n} v_n.
\end{array}
$$
We have to show that $x_1, \dots, x_{n+1}$ are linearly dependent.
Let us consider the first line. If $\alpha_{1,1} = \alpha_{1,2} = \cdots = \alpha_{1,n} = 0$, then $x_1 = 0$ which gives then that $x_1, \dots, x_{n+1}$ are linearly dependent.
Otherwise, there exists $j$ such that $\alpha_{1,j} \neq 0$. Without loss of generality we can assume that $\alpha_{1,1} \neq 0$. 
$$
\begin{array}{ccccccc}
	x_1 &=& \alpha_{1,1} v_1 &+& \cdots &+& \alpha_{1,n} v_n\\
	x_2 - \frac{\alpha_{2,1}}{\alpha_{1,1}} x_1 &=& 0 &+& \cdots &+& \alpha_{2,n} v_n - \frac{\alpha_{2,1}}{\alpha_{1,1}} \alpha_{1,n}v_n \\
	\vdots &&&&&& \\
	x_{n+1} - \frac{\alpha_{n+1,1}}{\alpha_{1,1}} x_1&=& 0 &+& \cdots &+& \alpha_{n+1,n} v_n- \frac{\alpha_{n+1,1}}{\alpha_{1,1}} \alpha_{1,n}v_n.
\end{array}
$$
If we define $y_i \defeq x_i - \frac{\alpha_{i,1}}{\alpha_{1,1}} x_1$ for $i = 2, \dots, n+1$ we obtain have $y_i \in \Span(v_2, \dots, v_{n})$. We can now apply the induction hypothesis $\mathcal{H}_{n-1}$ to get that $y_2, \dots, y_{n+1}$ are linearly dependent. This means that there exists $\beta_2, \dots \beta_{n+1}$ that are not all zero, such that $\beta_2 y_2 + \cdots + \beta_{n+1} y_{n+1} = 0$ which finally gives
$$
\Big(- \beta_2 \frac{\alpha_{2,1}}{\alpha_1,1} - \cdots - \beta_{n+1} \frac{\alpha_{n+1,1}}{\alpha_{1,1}}\Big) x_1 + \beta_2 x_2 + \cdots + \beta_{n+1} x_{n+1} = 0.
$$
Since $\beta_2, \dots, \beta_{n+1}$ are not all zero we get that $x_1, \dots, x_{n+1}$ are linearly dependent. $\mathcal{H}_{n}$ is proved.

\vspace{1cm}
\centerline{\pgfornament[width=7cm]{71}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}
